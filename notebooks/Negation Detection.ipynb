{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import foodbornenyc.models.models as models\n",
    "from foodbornenyc.models.businesses import Business, business_category_table\n",
    "from foodbornenyc.models.documents import YelpReview, Tweet, Document\n",
    "from foodbornenyc.models.locations import Location\n",
    "from foodbornenyc.models.metadata import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "f = xlrd.open_workbook('data/yelp_sick_classifier_data.xlsx')\n",
    "sheet1 = f.sheet_by_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        ...alty='l2', random_state=57, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "from foodbornenyc.settings import yelp_classify_config as config\n",
    "\n",
    "sick = joblib.load(\"../\"+config['model_file'])\n",
    "sick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def analyze(reviews, classifier):\n",
    "    textonly = [a[\"text\"] for a in reviews]\n",
    "    predictions_new = classifier.predict_proba(textonly)\n",
    "    label = np.array([review['label'] for review in reviews])\n",
    "    pred = np.array([pred[1] for pred in predictions_new])\n",
    "    print \"ROC_AUC SCORE ::\", roc_auc_score(label, pred, average='micro')\n",
    "    # determine true/false positive/negative rates\n",
    "    tp_rate = 0.0\n",
    "    fp_rate = 0.0\n",
    "    tn_rate = 0.0\n",
    "    fn_rate = 0.0\n",
    "\n",
    "    for review, pred in zip(reviews, predictions_new):\n",
    "        if review['label'] == 1.0 and pred[1] > 0.5: tp_rate += 1\n",
    "        elif review['label'] == 1.0 and pred[1] < 0.5: fn_rate += 1\n",
    "        elif review['label'] == 0.0 and pred[1] > 0.5: fp_rate += 1\n",
    "        elif review['label'] == 0.0 and pred[1] < 0.5: tn_rate += 1\n",
    "    tp_rate /= len(reviews)\n",
    "    fn_rate /= len(reviews)\n",
    "    fp_rate /= len(reviews)\n",
    "    tn_rate /= len(reviews)\n",
    "    print \"True positive ::\", tp_rate\n",
    "    print \"False negative ::\", fn_rate\n",
    "    print \"False positive ::\", fp_rate\n",
    "    print \"True negative ::\", tn_rate\n",
    "    print \"FP / TP ::\", fp_rate / tp_rate\n",
    "    print \"FN / TN ::\", fn_rate / tn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for i, (rev, label) in enumerate(zip(sheet1.col(1), sheet1.col(2))):\n",
    "    if i == 0: continue\n",
    "    reviews.append({\"text\":rev.value, \"label\":label.value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.996294837238\n",
      "True positive :: 0.523706896552\n",
      "False negative :: 0.00933908045977\n",
      "False positive :: 0.00646551724138\n",
      "True negative :: 0.460488505747\n",
      "FP / TP :: 0.0123456790123\n",
      "FN / TN :: 0.0202808112324\n"
     ]
    }
   ],
   "source": [
    "analyze(reviews, sick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sheet2 = xlrd.open_workbook('data/sick_test_preds.xlsx').sheet_by_index(0)\n",
    "reviews2 = []\n",
    "for i, (rev, label) in enumerate(zip(sheet2.col(0), sheet2.col(3))):\n",
    "    if i == 0: continue\n",
    "    reviews2.append({\"text\":rev.value, \"label\":label.value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.999638616417\n",
      "True positive :: 0.530465949821\n",
      "False negative :: 0.00358422939068\n",
      "False positive :: 0.0143369175627\n",
      "True negative :: 0.451612903226\n",
      "FP / TP :: 0.027027027027\n",
      "FN / TN :: 0.00793650793651\n"
     ]
    }
   ],
   "source": [
    "analyze(reviews2, sick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('count',\n",
       "  CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "          dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "          lowercase=True, max_df=0.95, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('tfidf',\n",
       "  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('log',\n",
       "  LogisticRegression(C=100, class_weight=None, dual=True, fit_intercept=True,\n",
       "            intercept_scaling=0.01, max_iter=100, multi_class='ovr',\n",
       "            n_jobs=1, penalty='l2', random_state=57, solver='liblinear',\n",
       "            tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sick.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj hope  []\n",
      "hope ROOT hope  [u'I', u'gets', u'.']\n",
      "none nsubj gets  [u'of']\n",
      "of prep none  [u'us']\n",
      "us pobj of  []\n",
      "gets ccomp hope  [u'none', u'sick', u'tonight']\n",
      "sick acomp gets  []\n",
      "tonight npadvmod gets  []\n",
      ". punct hope  []\n",
      "I nsubj order  []\n",
      "did aux order  []\n",
      "n't neg order  []\n",
      "order ROOT order  [u'I', u'did', u\"n't\", u'poisoning', u'.']\n",
      "food compound poisoning []\n",
      "poisoning dobj order  [u'food']\n",
      ". punct order  []\n",
      "I nsubj think  []\n",
      "do aux think  []\n",
      "not neg think  []\n",
      "think ROOT think  [u'I', u'do', u'not', u'come', u'.']\n",
      "you nsubj come  []\n",
      "should aux come  []\n",
      "come ccomp think  [u'you', u'should', u'here', u'got']\n",
      "here advmod come  []\n",
      "because mark got  []\n",
      "I nsubj got  []\n",
      "got advcl come  [u'because', u'I', u'poisoning']\n",
      "food compound poisoning []\n",
      "poisoning dobj got  [u'food']\n",
      ". punct think  []\n"
     ]
    }
   ],
   "source": [
    "# key words to watch out for: poisoning, sick, \n",
    "# tokens that perform strictly negation: not, n't, no, none, nobody, neither, \n",
    "# if negation word's head == key word's head, prepend key word with \"not\" and remove negation word\n",
    "# only potential issue is double negative, e.g. \"no one didn't get food poisoning\", but this is a first step\n",
    "from spacy import attrs\n",
    "example = u\"I hope none of us gets sick tonight. I didn't order food poisoning. I do not think you should come here because I got food poisoning.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print token.orth_, token.dep_, token.head, [t.orth_ for t in token.children]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hope none of us gets sick tonight . i did order food not poisoning . i do think you should come here because i got food not poisoning .\n"
     ]
    }
   ],
   "source": [
    "def transform_doc_1(doc): \n",
    "    #if root of sentence had negation and sentence contained kw\n",
    "    kw = ['poisoning', 'sick']\n",
    "    neg = ['not', \"n't\", 'no', 'none', 'nobody', 'neither']\n",
    "    parsedDoc = parser(doc.lower())\n",
    "    tokens = [[t.orth_ for t in s] for s in parsedDoc.sents] #this will be modified\n",
    "    sents = list(parsedDoc.sents)\n",
    "    for i in range(len(sents)):\n",
    "        # each span has only one root\n",
    "        if not any([c.orth_ in neg for c in sents[i].root.children]): continue #if there's no negation\n",
    "        neg_i = [j for j in range(len(sents[i])) if sents[i][j].orth_ in neg][0]\n",
    "        kw_list = [j for j in range(len(sents[i])) if sents[i][j].orth_ in kw]\n",
    "        \n",
    "        if len(kw_list) == 0: continue\n",
    "        \n",
    "        kw_i = kw_list[0]\n",
    "        \n",
    "        #now modify\n",
    "        tokens[i].insert(kw_i, \"not\")\n",
    "        tokens[i].pop(neg_i)\n",
    "    #now we join everything with spaces\n",
    "    out = []\n",
    "    for sent in tokens:\n",
    "        out.append(\" \".join(sent))\n",
    "    return \" \".join(out)\n",
    "\n",
    "print transform_doc_1(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i hope of us gets not sick tonight . i did order food not poisoning . i do not think you should come here because i got food poisoning .\n"
     ]
    }
   ],
   "source": [
    "def transform_doc_2(doc): \n",
    "    #if negation and kw share a head\n",
    "    kw = ['poisoning', 'sick']\n",
    "    neg = ['not', \"n't\", 'no', 'none', 'nobody', 'neither']\n",
    "    parsedDoc = parser(doc.lower())\n",
    "    tokens = [[t.orth_ for t in s] for s in parsedDoc.sents] #this will be modified\n",
    "    sents = list(parsedDoc.sents)\n",
    "    for i in range(len(sents)):\n",
    "        if not any([c.orth_ in neg for c in sents[i]]): continue #if there's no negation\n",
    "        neg_list = [j for j in range(len(sents[i])) if sents[i][j].orth_ in neg]\n",
    "        kw_list = [j for j in range(len(sents[i])) if sents[i][j].orth_ in kw]\n",
    "        \n",
    "        if len(kw_list) == 0: continue\n",
    "        \n",
    "        # attempt at handling double negatives\n",
    "        double_negative = True\n",
    "        kw_i = -1\n",
    "        neg_i = -1\n",
    "        \n",
    "        for j in neg_list:\n",
    "            for k in kw_list:\n",
    "                if sents[i][j].head == sents[i][k].head and double_negative:\n",
    "                    neg_i = j\n",
    "                    kw_i = k\n",
    "                    double_negative = False\n",
    "                elif sents[i][j].head == sents[i][k].head and not double_negative:\n",
    "                    double_negative = True\n",
    "        \n",
    "        if double_negative: continue\n",
    "        \n",
    "        #now modify\n",
    "        tokens[i].insert(kw_i, \"not\")\n",
    "        tokens[i].pop(neg_i)\n",
    "    #now we join everything with spaces\n",
    "    out = []\n",
    "    for sent in tokens:\n",
    "        out.append(\" \".join(sent))\n",
    "    return \" \".join(out)\n",
    "\n",
    "print transform_doc_2(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class NegationTransformer(TransformerMixin):\n",
    "    \"\"\" Brings negation words closer to relevant key terms to make it detectable with n-gram detector \"\"\"\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return np.array([transform_doc_2(doc) for doc in X])\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('negTransformer', <__main__.NegationTransformer object at 0x175ce7150>), ('oldPipe', Pipeline(steps=[('count', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.95,...y='l2', random_state=57, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('negTransformer', NegationTransformer()), ('oldPipe', sick)])\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.996213974705\n",
      "True positive :: 0.524425287356\n",
      "False negative :: 0.00862068965517\n",
      "False positive :: 0.00718390804598\n",
      "True negative :: 0.459770114943\n",
      "FP / TP :: 0.013698630137\n",
      "FN / TN :: 0.01875\n",
      "CPU times: user 20.6 s, sys: 1.48 s, total: 22 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%time analyze(reviews, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.999638616417\n",
      "True positive :: 0.530465949821\n",
      "False negative :: 0.00358422939068\n",
      "False positive :: 0.0179211469534\n",
      "True negative :: 0.448028673835\n",
      "FP / TP :: 0.0337837837838\n",
      "FN / TN :: 0.008\n",
      "CPU times: user 4.1 s, sys: 45.7 ms, total: 4.15 s\n",
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%time analyze(reviews2, pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#try refitting the pipeline?\n",
    "pipe1 = Pipeline([('negTransformer', NegationTransformer()), ('oldPipe', sick)])\n",
    "pipe2 = Pipeline([('negTransformer', NegationTransformer()), ('oldPipe', sick)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fitting against reviews from scratch\n",
    "from sklearn import cross_validation\n",
    "data = {}\n",
    "data['X'] = [review['text'] for review in reviews]\n",
    "data['y'] = [review['label'] for review in reviews]\n",
    "folds = cross_validation.StratifiedKFold(data['y'], n_folds=3, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from yelp classifier training notebook\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def my_roc_auc(ground_truth, predictions):\n",
    "    ground_truth = np.array(ground_truth)\n",
    "    predictions = np.array(predictions)\n",
    "    return metrics.roc_auc_score(ground_truth, predictions, average='micro')\n",
    "\n",
    "my_roc_auc_scorer = metrics.make_scorer(my_roc_auc, needs_threshold=True, greater_is_better=True)\n",
    "# Feature Extractors\n",
    "cv = CountVectorizer(\n",
    "        input=u'content', \n",
    "        encoding=u'utf-8', \n",
    "        decode_error=u'strict', \n",
    "        strip_accents='unicode', \n",
    "        lowercase=True,\n",
    "        analyzer=u'word', \n",
    "        preprocessor=None, \n",
    "        tokenizer=None, \n",
    "        stop_words='english', \n",
    "        #token_pattern=u'(?u)\\\\b\\w\\w+\\b', # one alphanumeric is a token\n",
    "        ngram_range=(1, 2), \n",
    "        max_df=.9, \n",
    "        min_df=2, \n",
    "        max_features=None, \n",
    "        vocabulary=None, \n",
    "        binary=False, \n",
    "        #dtype=type 'numpy.int64'>\n",
    "        )\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf = TfidfTransformer(\n",
    "        norm='l2',\n",
    "        use_idf=True,\n",
    "        smooth_idf=True,\n",
    "        sublinear_tf=False\n",
    ")\n",
    "\n",
    "# Final Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "lr = LogisticRegression(C=.05,\n",
    "                        fit_intercept=True,\n",
    "                        random_state=0,\n",
    "                        class_weight='balanced',\n",
    "                        n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count', cv),\n",
    "    ('tfidf', tf),\n",
    "    ('logreg', lr)\n",
    "    ])\n",
    "\n",
    "param_grid = {\n",
    "    'count__ngram_range':[(1,1),(1,2),(1,3)],\n",
    "    'tfidf__norm':['l1', 'l2'],\n",
    "    'tfidf__use_idf':[True, False],\n",
    "    'tfidf__sublinear_tf':[True,False],\n",
    "    'logreg__C':[.001, .01, .1]\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           param_grid,\n",
    "                           cv = folds,\n",
    "                           scoring=my_roc_auc_scorer,\n",
    "                           n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Best score: 0.875\n",
      "Best parameters set:\n",
      "\tcount__ngram_range: (1, 3)\n",
      "\tlogreg__C: 0.1\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__sublinear_tf: True\n",
      "\ttfidf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(np.array(data['X']), data['y'])\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.962345013477\n",
      "True positive :: 0.451149425287\n",
      "False negative :: 0.0818965517241\n",
      "False positive :: 0.0337643678161\n",
      "True negative :: 0.433189655172\n",
      "FP / TP :: 0.0748407643312\n",
      "FN / TN :: 0.189054726368\n",
      "CPU times: user 867 ms, sys: 23.4 ms, total: 890 ms\n",
      "Wall time: 908 ms\n"
     ]
    }
   ],
   "source": [
    "%time analyze(reviews, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC_AUC SCORE :: 0.960247805885\n",
      "True positive :: 0.469534050179\n",
      "False negative :: 0.0645161290323\n",
      "False positive :: 0.0465949820789\n",
      "True negative :: 0.41935483871\n",
      "FP / TP :: 0.0992366412214\n",
      "FN / TN :: 0.153846153846\n",
      "CPU times: user 183 ms, sys: 13.1 ms, total: 197 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%time analyze(reviews2, grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
